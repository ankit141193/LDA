{"nbformat_minor": 2, "cells": [{"source": "# Latent Dirichlet Allocation", "cell_type": "markdown", "metadata": {}}, {"source": "Latent Dirichlet Allocation(LDA) is a way of automatically discovering topics that exist in text. You can read more about it here https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation", "cell_type": "markdown", "metadata": {}}, {"source": "Importing required components", "cell_type": "markdown", "metadata": {}}, {"execution_count": 290, "cell_type": "code", "source": "from collections import defaultdict\nfrom pyspark import SparkContext\nfrom pyspark.mllib.linalg import Vector, Vectors\nfrom pyspark.mllib.clustering import LDA, LDAModel\nfrom pyspark.sql import SQLContext\nimport re", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Kindly verify if the SparkContext and SQLContext have been created", "cell_type": "markdown", "metadata": {}}, {"source": "## Importing the data", "cell_type": "markdown", "metadata": {}}, {"source": "The current input dataset conisist of reviews for an airline scraped from TripAdvisor.com. I used https://www.import.io/ to scrape the data, it is a pretty nifty tool for tasks when writing a scraper is not required. The scraped data was stored in an excel which contained one review per row. The dataset was then uploaded to Azure storage service.", "cell_type": "markdown", "metadata": {}}, {"source": "Reading dataset from Azure Blob, kindly replace the path based on where your data is stored", "cell_type": "markdown", "metadata": {}}, {"execution_count": 291, "cell_type": "code", "source": "data=sc.wholeTextFiles('wasb:///Reviews.csv')", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Currently all the reviews are in a single tuple and we need to parse them into an RDD", "cell_type": "markdown", "metadata": {}}, {"execution_count": 292, "cell_type": "code", "source": "## Extracting the text from the tuple into an RDD\ndata=data.map(lambda line: line[1])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 293, "cell_type": "code", "source": "## tokenize the data to form our global vocabulary\ntokens = data.map( lambda document: document.strip().lower()).map( lambda document: re.split(\"[\\s;,#]\", document))\\\n.map( lambda word: [x for x in word if x.isalpha()]).map( lambda word: [x for x in word if len(x) > 3] )", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Setting up parameters, all of these are modifiable and should be tuned to obtain best possible results", "cell_type": "markdown", "metadata": {}}, {"execution_count": 294, "cell_type": "code", "source": "## Defining our thresholds \nnum_of_stop_words = 70      # Number of most common words to remove, trying to eliminate stop words\nnum_topics = 4              # Number of topics we are looking for\nnum_words_per_topic = 10    # Number of words to display for each topic\nmax_iterations = 35         # Max number of times to iterate before finishing_\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Put all the words in one list instead of a list per document", "cell_type": "markdown", "metadata": {}}, {"execution_count": 295, "cell_type": "code", "source": "termCounts = tokens.flatMap(lambda document: document)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Mapping 1 to each entry", "cell_type": "markdown", "metadata": {}}, {"execution_count": 296, "cell_type": "code", "source": "termCounts=termCounts.map(lambda word: (word, 1))\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Merging all the tuples together by the word, to get count of each word", "cell_type": "markdown", "metadata": {}}, {"execution_count": 297, "cell_type": "code", "source": "termCounts=termCounts.reduceByKey( lambda x,y: x + y)\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Reversing the columns and sorting by word count ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 298, "cell_type": "code", "source": "termCounts=termCounts.map(lambda tuple: (tuple[1], tuple[0])).sortByKey(False)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Verifying if the intended result was obtained", "cell_type": "markdown", "metadata": {}}, {"execution_count": 299, "cell_type": "code", "source": "termCounts.take(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(41, u'flight'), (26, u'have'), (24, u'were'), (22, u'country'), (22, u'with')]"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "Identify a threshold to remove the top words, in an effort to remove stop words.\nThis is the wordcount above which all words will be dropped", "cell_type": "markdown", "metadata": {}}, {"execution_count": 300, "cell_type": "code", "source": "threshold_value = termCounts.take(num_of_stop_words)[num_of_stop_words - 1][0]\nthreshold_value", "outputs": [{"output_type": "stream", "name": "stdout", "text": "4"}], "metadata": {"collapsed": false}}, {"source": "Retain words with a count less than the threshold identified above\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 301, "cell_type": "code", "source": "vocabulary = termCounts.filter(lambda x : x[0] < threshold_value)  ", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Index each word and collect them into a map", "cell_type": "markdown", "metadata": {}}, {"execution_count": 302, "cell_type": "code", "source": "vocabulary =vocabulary.map(lambda x: x[1]).zipWithIndex().collectAsMap()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Convert the given documents into a sparse vector of word counts for each document\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 303, "cell_type": "code", "source": "\ndef document_vector(document):\n    id = document[1]\n    counts = defaultdict(int)\n    for token in document[0]:\n        if token in vocabulary:\n            token_id = vocabulary[token]\n            counts[token_id] += 1\n    counts = sorted(counts.items())\n    keys = [x[0] for x in counts]\n    values = [x[1] for x in counts]\n    return (id, Vectors.sparse(len(vocabulary), keys, values))", "outputs": [], "metadata": {"collapsed": false}}, {"source": " Process all of the documents into word vectors using the function defined above\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 304, "cell_type": "code", "source": "documents = tokens.zipWithIndex().map(document_vector).map(list)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Inverting the key value to get index value\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 305, "cell_type": "code", "source": "inv_voc = {value: key for (key, value) in vocabulary.items()}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Print topics, showing the top-weighted 10 terms for each topic\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 306, "cell_type": "code", "source": "lda_model = LDA.train(documents, k=num_topics, maxIterations=max_iterations)\ntopic_indices = lda_model.describeTopics(maxTermsPerTopic=num_words_per_topic)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Print topics, showing the top-weighted 10 terms for each topic\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 307, "cell_type": "code", "source": "for i in range(len(topic_indices)):\n    print(\"Topic #{0}\\n\".format(i + 1))\n    for j in range(len(topic_indices[i][0])):\n        print(\"{0}\\t{1}\\n\".format(inv_voc[topic_indices[i][0][j]].encode('utf-8'), topic_indices[i][1][j]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Topic #1\n\nlittle\t0.00483095938762\n\nticket\t0.0048097929714\n\nable\t0.00478491508584\n\ntrip\t0.00477243470492\n\nfront\t0.00476783161698\n\nmake\t0.00476645514384\n\ntook\t0.00476613505118\n\neverything\t0.00476562503125\n\nmexico\t0.00476287139118\n\nwent\t0.00476264789923\n\nTopic #2\n\nnothing\t0.00481264620268\n\ncrew\t0.0047941485926\n\nmother\t0.00479139674775\n\nlate\t0.00477583625011\n\nplace\t0.00477561386554\n\nthink\t0.00477426868352\n\nvacation\t0.00477048478905\n\nplanes\t0.00476555238096\n\ncancun\t0.00476369097919\n\npolicy\t0.00476106600818\n\nTopic #3\n\nenjoyed\t0.00490285907102\n\nlots\t0.00489043595073\n\nbeing\t0.00489041159345\n\nbeen\t0.00487855517466\n\neverything\t0.00487622501074\n\nalso\t0.004869001319\n\nhours\t0.00486504201246\n\nairlines\t0.00486327114898\n\nplace\t0.00485611305511\n\nbaggage\t0.0048560506319\n\nTopic #4\n\nbest\t0.00477695062697\n\nwedding\t0.00477686022169\n\nnothing\t0.00477504328173\n\nmost\t0.00476701780228\n\nalso\t0.00476522251223\n\nappreciate\t0.00476386642255\n\ntook\t0.00475527292484\n\nwent\t0.00474700941014\n\nplane\t0.00474679767216\n\nseemed\t0.0047455309842"}], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": 308, "cell_type": "code", "source": "print(\"There are {0} topics over {1} documents and {2} unique words\\n\" \\\n              .format(num_topics, documents.count(), len(vocabulary)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "There are 4 topics over 1 documents and 479 unique words"}], "metadata": {"collapsed": false}}, {"source": "As we can see from the topics above there is in general a positive sentiment towards the airline.\n\nOne important point is that LDA does not give names to the topic it only gives the words which belong to the topic. Naming the topic and interpreting them is a human task", "cell_type": "markdown", "metadata": {}}, {"source": "## Next Steps", "cell_type": "markdown", "metadata": {}}, {"source": "1. The above analysis was run using a sample of 50 reviews. This can be expanded to get a more generalized view.\n2. Airline reviews are available form multiple sources besides TripAdvisor which can be added to the data source. Again https://www.import.io/ is a great tool to scrape websites and it supports a wide variety of websites.\n3. The same code can be used for text analysis of any documents and find the topics that exist.", "cell_type": "markdown", "metadata": {}}, {"source": "The above analysis was run on a spark HDInsight cluster on Azure with the following configuration.\n\nHead nodes:D12 v2 (x2), worker nodes: D12 v2 (x2)\n\nSpark version: 2.2", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}